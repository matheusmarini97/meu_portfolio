# üìÑ Projeto de Estudo ‚Äì DAGs no Apache Airflow

Este reposit√≥rio cont√©m um conjunto de DAGs desenvolvidas como **projeto de estudo em Apache Airflow**.  
O objetivo principal foi praticar conceitos de **ETL, orquestra√ß√£o de tarefas, integra√ß√£o com bancos de dados, manipula√ß√£o de arquivos CSV e extra√ß√£o de dados de APIs externas**.

Cada DAG apresentada neste projeto tem um prop√≥sito espec√≠fico, seja replica√ß√£o de dados entre bancos MySQL, transforma√ß√£o de arquivos CSV, gera√ß√£o de relat√≥rios, ou extra√ß√£o de dados financeiros e meteorol√≥gicos.  
Todas as DAGs foram criadas para fins de aprendizado e experimenta√ß√£o, servindo como exemplos pr√°ticos do uso do Airflow em diferentes cen√°rios de integra√ß√£o e processamento de dados.

---

## üü¶ DAG 01 ‚Äì `apiDag`

A DAG **`apiDag`** tem como objetivo realizar um fluxo simples de tarefas que envolve a prepara√ß√£o de diret√≥rios para armazenar arquivos de execu√ß√£o di√°ria.  
Ela cria uma estrutura de pastas nomeadas dinamicamente com base no `data_interval_end`, garantindo que cada execu√ß√£o do pipeline tenha um diret√≥rio pr√≥prio e organizado.

Essa DAG funciona como uma etapa de **prepara√ß√£o e organiza√ß√£o do ambiente**, servindo de suporte para outros processos que dependem de diret√≥rios estruturados antes de iniciar suas atividades.

---
## üü¶ DAG 02 ‚Äì `espelhar_tabelas`

A DAG **`espelhar_tabelas`** tem como objetivo realizar a c√≥pia de dados entre dois bancos MySQL distintos.  
Ela l√™ registros da tabela `tb01_tipo_perfil` no banco de origem e replica esses dados no banco de destino, garantindo que ambas as bases permane√ßam sincronizadas.

Essa DAG funciona como um processo de **espelhamento e integra√ß√£o de dados** entre sistemas, permitindo manter consist√™ncia entre duas bases MySQL de forma automatizada.


---

## üü¶ DAG 03 ‚Äì `etl_csv_to_tb02_user`

A DAG **`etl_csv_to_tb02_user`** tem como objetivo importar dados a partir de um arquivo CSV e carreg√°-los na tabela `tb02_user` em um banco MySQL.  
Ela l√™ o arquivo localizado em `/opt/airflow/csv/tb02_user.csv`, converte campos vazios para valores nulos (NULL) e insere cada registro de forma estruturada na tabela de destino.

Essa DAG funciona como um processo de **ingest√£o e saneamento de dados**, garantindo que informa√ß√µes vindas de arquivos CSV sejam carregadas corretamente no banco e padronizadas antes do armazenamento.

---

## üü¶ DAG 04 ‚Äì `etl_tb02_user`

A DAG **`etl_tb02_user`** tem como objetivo copiar dados da tabela `tb02_user` entre dois bancos MySQL diferentes.  
Ela consulta todos os registros no banco de origem e realiza a inser√ß√£o desses mesmos dados no banco de destino, preservando a estrutura completa da tabela e seus campos.

Essa DAG atua como um processo de **replica√ß√£o e sincroniza√ß√£o de dados**, garantindo que a tabela `tb02_user` seja mantida atualizada e consistente entre dois ambientes distintos.

---

## üü¶ DAG 05 ‚Äì `etl_tb02_user_to_csv`

A DAG **`etl_tb02_user_to_csv`** tem como objetivo exportar dados da tabela `Aniversariantes` presente em um banco MySQL e salvar esses registros em um arquivo CSV.  
Ela realiza a consulta completa da tabela, gera o arquivo `tb02_user.csv` no diret√≥rio `/opt/airflow/csv/` e inclui tanto o cabe√ßalho quanto todos os dados retornados na consulta.

Essa DAG funciona como um processo de **extra√ß√£o e gera√ß√£o de arquivo CSV**, facilitando o uso dos dados exportados em outras ferramentas, an√°lises ou integra√ß√µes externas.

---

## üü¶ DAG 06 ‚Äì `executePipeline`

A DAG **`executePipeline`** tem como objetivo orquestrar a execu√ß√£o de um pipeline composto por outras duas DAGs do Airflow.  
Ela dispara, de forma sequenciada, a DAG respons√°vel pela extra√ß√£o dos aniversariantes para CSV (`extractAniversariantesCsv`) e, ap√≥s sua conclus√£o, aciona a DAG de transforma√ß√£o e carga (`transformLoadAniversariantesCsv`), aguardando a finaliza√ß√£o de cada etapa antes de prosseguir.

Essa DAG funciona como um **pipeline controlador**, garantindo que o fluxo de extra√ß√£o, transforma√ß√£o e carga seja executado de maneira coordenada, segura e na ordem correta.

---
## üü¶ DAG 07 ‚Äì `extractAniversariantesCsv`

A DAG **`extractAniversariantesCsv`** tem como objetivo extrair os dados da tabela `Aniversariantes` de um banco MySQL e gerar um arquivo CSV com essas informa√ß√µes.  
Ela realiza a consulta completa da tabela, cria o arquivo `aniversariantes.csv` no diret√≥rio `/opt/airflow/csv/` e inclui tanto o cabe√ßalho quanto os registros obtidos.

Essa DAG funciona como um processo de **extra√ß√£o e exporta√ß√£o de dados**, permitindo que informa√ß√µes de aniversariantes estejam dispon√≠veis em formato CSV para outras etapas do pipeline ou sistemas externos.

---
## üü¶ DAG 08 ‚Äì `transformLoadAniversariantesCsv`

A DAG **`transformLoadAniversariantesCsv`** tem como objetivo transformar os dados contidos no arquivo CSV `aniversariantes.csv` e carreg√°-los em uma tabela MySQL chamada `aniversariantes`.  
Ela realiza limpeza e filtragem dos dados, remove colunas desnecess√°rias, exclui duplicidades, agrupa turmas, converte datas de nascimento para o formato de m√™s por extenso e ordena os registros antes da inser√ß√£o.

Essa DAG funciona como um processo de **transforma√ß√£o e carga (ETL)**, garantindo que os dados exportados anteriormente estejam estruturados, consistentes e prontos para an√°lise ou consumo por outros sistemas.

---
## üü¶ DAG 09 ‚Äì `weatherDag`

A DAG **`weatherDag`** tem como objetivo extrair dados meteorol√≥gicos semanais de Londres, Reino Unido, a partir da API Visual Crossing, e armazen√°-los em arquivos CSV.  
Ela cria um diret√≥rio espec√≠fico para cada semana, salva os dados brutos, al√©m de gerar arquivos separados contendo apenas temperaturas e condi√ß√µes meteorol√≥gicas.

Essa DAG funciona como um processo de **extra√ß√£o e organiza√ß√£o de dados meteorol√≥gicos**, permitindo que informa√ß√µes de temperatura e condi√ß√µes clim√°ticas estejam dispon√≠veis de forma estruturada para an√°lises ou integra√ß√µes futuras.

---

## üü¶ DAG 10 ‚Äì `get_stocks_dag`

A DAG **`get_stocks_dag`** tem como objetivo extrair dados hist√≥ricos de a√ß√µes de empresas espec√≠ficas (AAPL, MSFT, GOOG e TSLA) utilizando a biblioteca **yfinance**.  
Ela coleta informa√ß√µes com intervalo de 1 hora, cria diret√≥rios separados para cada a√ß√£o e salva os dados em arquivos CSV di√°rios.

Essa DAG funciona como um processo de **extra√ß√£o e armazenamento de dados financeiros**, permitindo que informa√ß√µes de mercado estejam organizadas e dispon√≠veis para an√°lises, monitoramento ou integra√ß√£o em outros pipelines.
